{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2bb8d3",
   "metadata": {},
   "source": [
    "# Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4394847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import NNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from utils.time import name_to_day\n",
    "from data_processing import (\n",
    "    load_train_test_split,\n",
    "    load_graph,\n",
    "    load_delays,\n",
    "    load_stop_id_to_idx,\n",
    "    load_trip_idx_list,\n",
    "    available_bin_codes\n",
    ")\n",
    "\n",
    "from train import (\n",
    "    load_scalers, \n",
    "    load_train_data\n",
    ")\n",
    "root = Path('/mnt/c/Users/rdsup/Desktop/vitmma19-pw-delay-detection')\n",
    "\n",
    "data_path = root / 'data'\n",
    "delays_path = data_path / 'delays'\n",
    "graphs_path = data_path / 'graphs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51d6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_idx_list = load_trip_idx_list(graphs_path)\n",
    "records = load_train_test_split(data_path)\n",
    "\n",
    "record_name = '20251011'\n",
    "day = name_to_day(record_name)[0] # 0-6 TODO: tensor\n",
    "df = load_delays(record_name,delays_path)\n",
    "bin_codes = available_bin_codes(record_name, graphs_path)\n",
    "\n",
    "dt = 1/2\n",
    "bins = np.arange(0,24+dt,dt)\n",
    "bins *= 60*60\n",
    "\n",
    "df['bin'] = pd.cut(df['trip_start'], bins=bins, right=False)\n",
    "df['bin_code'] = df['bin'].cat.codes\n",
    "df = df[df['bin_code'].isin(bin_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b27f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_id_list = df.index.to_numpy()\n",
    "sec_list = torch.tensor(df['trip_start'].to_numpy(),dtype=torch.float32)\n",
    "day_list = torch.ones_like(sec_list,dtype=torch.float32)*day\n",
    "delay_list = torch.tensor(df['delay'].to_numpy(),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109c7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    x_train, \n",
    "    edge_attr_train, \n",
    "    delays_train\n",
    ") = load_train_data(data_path)\n",
    "scalers = load_scalers(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65894f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1b2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes)-2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GCNWithEdge(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], edge_attr_dim: int):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            nn_edge = nn.Sequential(\n",
    "                nn.Linear(edge_attr_dim, layer_sizes[i]*layer_sizes[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer_sizes[i]*layer_sizes[i+1], layer_sizes[i]*layer_sizes[i+1])\n",
    "            )\n",
    "            self.convs.append(NNConv(layer_sizes[i], layer_sizes[i+1], nn_edge, aggr='mean'))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EndOfTripDelay(nn.Module):\n",
    "    def __init__(self, \n",
    "            in_dim: int, \n",
    "            gcn_dims:list[int], edge_attr_dim: int, \n",
    "            embd_dim: int,\n",
    "            mlp_dims: list[int],\n",
    "            out_dim: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.GCN = GCNWithEdge(\n",
    "            [in_dim] + gcn_dims + [embd_dim], \n",
    "            edge_attr_dim\n",
    "        )\n",
    "        \n",
    "        self.MLP = MLP(\n",
    "            [embd_dim + 2] + mlp_dims + [out_dim]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, trip_nodes, day, sec):\n",
    "\n",
    "        # GCN\n",
    "        x = self.GCN(x, edge_index, edge_attr)\n",
    "\n",
    "        # Average trip pooling\n",
    "        x = x[trip_nodes].mean(dim=0)\n",
    "\n",
    "        # Meta embedding\n",
    "        x = torch.cat([x, day, sec])\n",
    "\n",
    "        # MLP for prediction\n",
    "        x = self.MLP(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f27808f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m \u001b[38;5;66;03m# continue\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m scalers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mg\u001b[49m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m     21\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m scalers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(g\u001b[38;5;241m.\u001b[39medge_attr)\n\u001b[1;32m     22\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39medge_index\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "model = EndOfTripDelay(\n",
    "    in_dim=7,gcn_dims=[32],edge_attr_dim=2,embd_dim=16,mlp_dims=[2],out_dim=1\n",
    ")\n",
    "\n",
    "#########--TRAIN ITER--##########\n",
    "i = 5\n",
    "trip_id = trip_id_list[i]\n",
    "###########################\n",
    "\n",
    "day = day_list[i].unsqueeze(0)\n",
    "sec = sec_list[i].unsqueeze(0)\n",
    "delay = delay_list[i].unsqueeze(0)\n",
    "try:\n",
    "    g = load_graph(sec, 1/2, record_name, graphs_path)\n",
    "    g.edge_attr.detach() # Static edges right now\n",
    "    trip_nodes = trip_idx_list[trip_id]\n",
    "except:\n",
    "    pass # continue\n",
    "\n",
    "x = scalers['x'].transform(g.x)\n",
    "edge_attr = scalers['edge_attr'].transform(g.edge_attr)\n",
    "edge_index = g.edge_index\n",
    "\n",
    "day = scalers['day'].transform(day)\n",
    "sec = scalers['sec'].transform(sec)\n",
    "delay = scalers['delay'].transform(delay)\n",
    "\n",
    "model(x, edge_index, edge_attr, trip_nodes, day, sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748df6c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b0d1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_train_test_split(data_path)\n",
    "record_name = '20251011'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502e2c5",
   "metadata": {},
   "source": [
    "### Single graph in cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, record_name, data_path:Path, scalers:dict):\n",
    "        \n",
    "        self.scalers = scalers\n",
    "        self.record_name = record_name\n",
    "        self.graphs_path = data_path / 'graphs'\n",
    "\n",
    "        # Nodes of the trips, by trip_id\n",
    "        self.nodes_of_trips = load_trip_idx_list(self.graphs_path)\n",
    "        \n",
    "        # Which day?\n",
    "        day = name_to_day(record_name)[0]\n",
    "        \n",
    "        # Load and filter (by available bins) delays\n",
    "        df = load_delays(record_name,delays_path)\n",
    "        df = self.filter_delays(df,dt=1/2)\n",
    "\n",
    "        # Training data\n",
    "        self.bin_codes = df['bin_code'].to_numpy()\n",
    "        self.bin_code = None\n",
    "        \n",
    "        self.trip_ids = df.index.to_numpy()\n",
    "\n",
    "        self.secs = scalers['sec'].transform(\n",
    "            torch.tensor(df['trip_start'].to_numpy(),dtype=torch.float32).unsqueeze(-1)\n",
    "        )\n",
    "        \n",
    "        day_tensor = torch.tensor([[day]], dtype=torch.float32)\n",
    "        self.days = scalers['day'].transform(day_tensor) * torch.ones_like(self.secs)\n",
    "        \n",
    "        self.delays = scalers['delay'].transform(\n",
    "            torch.tensor(df['delay'].to_numpy(),dtype=torch.float32).unsqueeze(-1)\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        trip_id = self.trip_ids[idx]\n",
    "        trip_nodes = self.nodes_of_trips[trip_id]\n",
    "\n",
    "        sec = self.secs[idx]\n",
    "        day = self.days[idx]\n",
    "        delay = self.delays[idx]\n",
    "        \n",
    "        if self.bin_codes[idx] != self.bin_code:\n",
    "            self.bin_code = self.bin_codes[idx]\n",
    "\n",
    "            # Load the graph\n",
    "            self.g = load_graph(\n",
    "                self.bin_code, \n",
    "                dt=1/2, \n",
    "                record_name=self.record_name, \n",
    "                graphs_path=self.graphs_path\n",
    "            )\n",
    "            self.x = self.scalers['x'].transform(self.g.x)\n",
    "            self.edge_attr = self.scalers['edge_attr'].transform(self.g.edge_attr)\n",
    "            self.edge_index = self.g.edge_index\n",
    "\n",
    "        return {\n",
    "            \"x\": self.x,\n",
    "            \"edge_index\": self.edge_index,\n",
    "            \"edge_attr\": self.edge_attr,\n",
    "            \"trip_nodes\": trip_nodes,\n",
    "            \"day\": day,\n",
    "            \"sec\": sec,\n",
    "            \"delay\": delay,\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trip_ids)\n",
    "\n",
    "    def filter_delays(self, df:pd.DataFrame, dt=1/2):\n",
    "        bin_codes = available_bin_codes(\n",
    "            self.record_name, self.graphs_path\n",
    "        )\n",
    "        bins = np.arange(0,24+dt,dt)\n",
    "        bins *= 60*60\n",
    "\n",
    "        df['bin'] = pd.cut(df['trip_start'], bins=bins, right=False)\n",
    "        df['bin_code'] = df['bin'].cat.codes\n",
    "        return df[df['bin_code'].isin(bin_codes)].sort_values('bin_code') # .groupby('bin_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa316a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = load_scalers(data_path)\n",
    "dataset = TripDataset(\n",
    "    record_name='20251011',\n",
    "    data_path=data_path,\n",
    "    scalers=scalers\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3884c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8884 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8884/8884 [00:01<00:00, 5138.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93b908",
   "metadata": {},
   "source": [
    "### Full graph cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94eb5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, record_name, data_path:Path, scalers:dict):\n",
    "        \n",
    "        self.scalers = scalers\n",
    "        self.record_name = record_name\n",
    "        self.graphs_path = data_path / 'graphs'\n",
    "\n",
    "        # Load graphs for the record\n",
    "        self.load_graph_cache() # before filtering delays!!!\n",
    "        \n",
    "        # Nodes of the trips, by trip_id\n",
    "        self.nodes_of_trips = load_trip_idx_list(self.graphs_path)\n",
    "        \n",
    "        # Which day?\n",
    "        day = name_to_day(record_name)[0]\n",
    "        \n",
    "        # Load and filter (by available bins) delays\n",
    "        df = load_delays(record_name,delays_path)\n",
    "        df = self.filter_delays(df,dt=1/2)\n",
    "\n",
    "        # Training data\n",
    "        self.bin_codes = df['bin_code'].to_numpy()\n",
    "        self.trip_ids = df.index.to_numpy()\n",
    "\n",
    "        self.secs = scalers['sec'].transform(\n",
    "            torch.tensor(df['trip_start'].to_numpy(),dtype=torch.float32).unsqueeze(-1)\n",
    "        )\n",
    "        \n",
    "        day_tensor = torch.tensor([[day]], dtype=torch.float32)\n",
    "        self.days = scalers['day'].transform(day_tensor) * torch.ones_like(self.secs)\n",
    "        \n",
    "        self.delays = scalers['delay'].transform(\n",
    "            torch.tensor(df['delay'].to_numpy(),dtype=torch.float32).unsqueeze(-1)\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        g = self.g_cache[self.bin_codes[idx]]\n",
    "\n",
    "        data = Data(\n",
    "            x=g.x,\n",
    "            edge_index=g.edge_index,\n",
    "            edge_attr=g.edge_attr,\n",
    "            y=self.delays[idx],\n",
    "            day=self.days[idx],\n",
    "            sec=self.secs[idx],\n",
    "            trip_nodes=self.nodes_of_trips[self.trip_ids[idx]]\n",
    "        )\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trip_ids)\n",
    "\n",
    "\n",
    "    def load_graph_cache(self):\n",
    "        self.g_cache = {}\n",
    "        for f in (self.graphs_path / self.record_name).iterdir():\n",
    "            if f.is_file() and f.name.startswith(\"graph_bin_\") and f.suffix == \".pt\":\n",
    "                bin_code = int(f.stem.replace(\"graph_bin_\", \"\"))\n",
    "                g = torch.load(f, weights_only=False)\n",
    "                g.x = self.scalers['x'].transform(g.x)\n",
    "                g.edge_attr = self.scalers['edge_attr'].transform(g.edge_attr)\n",
    "                self.g_cache[bin_code] = g\n",
    "            \n",
    "\n",
    "    def filter_delays(self, df:pd.DataFrame, dt=1/2):\n",
    "        bins = np.arange(0,24+dt,dt)\n",
    "        bins *= 60*60\n",
    "\n",
    "        df['bin'] = pd.cut(df['trip_start'], bins=bins, right=False)\n",
    "        df['bin_code'] = df['bin'].cat.codes\n",
    "        return df[df['bin_code'].isin(self.g_cache.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1224b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = load_scalers(data_path)\n",
    "dataset = TripDataset(\n",
    "    record_name='20251011',\n",
    "    data_path=data_path,\n",
    "    scalers=scalers\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae32daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 278/278 [00:00<00:00, 290.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c671b5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[90020, 7], edge_index=[2, 112740], edge_attr=[112740, 2], y=[20], day=[20], sec=[20], trip_nodes=[20], batch=[90020], ptr=[21])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb6bf5",
   "metadata": {},
   "source": [
    "# Train???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6f3641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EndOfTripDelay(\n",
       "  (GCN): GCNWithEdge(\n",
       "    (convs): ModuleList(\n",
       "      (0): NNConv(7, 32, aggr=mean, nn=Sequential(\n",
       "        (0): Linear(in_features=2, out_features=224, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=224, out_features=224, bias=True)\n",
       "      ))\n",
       "      (1): NNConv(32, 16, aggr=mean, nn=Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      ))\n",
       "    )\n",
       "  )\n",
       "  (MLP): MLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=18, out_features=2, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EndOfTripDelay(\n",
    "    in_dim=7,\n",
    "    gcn_dims=[32],\n",
    "    edge_attr_dim=2,\n",
    "    embd_dim=16,\n",
    "    mlp_dims=[2],\n",
    "    out_dim=1\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        # Move batch to device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # Assume your model accepts x, edge_index, edge_attr, trip_nodes, day, sec\n",
    "        pred = model(\n",
    "            batch.x,\n",
    "            batch.edge_index,\n",
    "            batch.edge_attr,\n",
    "            batch.trip_nodes,\n",
    "            batch.day,\n",
    "            batch.sec\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(pred, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c643dd",
   "metadata": {},
   "source": [
    "# With batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1d073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, record_name, data_path:Path, scalers:dict):\n",
    "        \n",
    "        self.scalers = scalers\n",
    "        self.record_name = record_name\n",
    "        self.graphs_path = data_path / 'graphs'\n",
    "        delays_path = data_path / 'delays'\n",
    "\n",
    "        # Load graphs for the record\n",
    "        self.load_graph_cache() # before filtering delays!!!\n",
    "        \n",
    "        # Nodes of the trips, by trip_id\n",
    "        self.nodes_of_trips = load_trip_idx_list(self.graphs_path)\n",
    "        \n",
    "        # Which day?\n",
    "        day = name_to_day(record_name)[0]\n",
    "        \n",
    "        # Load and filter (by available bins) delays\n",
    "        df = load_delays(record_name,delays_path)\n",
    "        df = self.filter_delays(df,dt=1/2)\n",
    "\n",
    "        # Training data\n",
    "        self.bin_codes = df['bin_code'].to_numpy()\n",
    "        self.trip_ids = df.index.to_numpy()\n",
    "\n",
    "        self.secs = scalers['sec'].transform(\n",
    "            torch.tensor(df['trip_start'].to_numpy(),dtype=torch.float32).unsqueeze(-1)\n",
    "        )\n",
    "        \n",
    "        day_tensor = torch.tensor([[day]], dtype=torch.float32)\n",
    "        self.days = scalers['day'].transform(day_tensor) * torch.ones_like(self.secs)\n",
    "        \n",
    "        self.delays = scalers['delay'].transform(\n",
    "            torch.tensor(df['delay'].to_numpy(),dtype=torch.float32).unsqueeze(-1)\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        g = self.g_cache[self.bin_codes[idx]]\n",
    "        num_nodes = g.x.size(0)\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[self.nodes_of_trips[self.trip_ids[idx]]] = True\n",
    "\n",
    "        data = Data(\n",
    "            x=g.x,\n",
    "            edge_index=g.edge_index,\n",
    "            edge_attr=g.edge_attr,\n",
    "            y=self.delays[idx],\n",
    "            day=self.days[idx],\n",
    "            sec=self.secs[idx],\n",
    "            trip_mask=mask,\n",
    "        )\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trip_ids)\n",
    "\n",
    "\n",
    "    def load_graph_cache(self):\n",
    "        self.g_cache = {}\n",
    "        for f in (self.graphs_path / self.record_name).iterdir():\n",
    "            if f.is_file() and f.name.startswith(\"graph_bin_\") and f.suffix == \".pt\":\n",
    "                bin_code = int(f.stem.replace(\"graph_bin_\", \"\"))\n",
    "                g = torch.load(f, weights_only=False)\n",
    "                g.x = self.scalers['x'].transform(g.x)\n",
    "                g.edge_attr = self.scalers['edge_attr'].transform(g.edge_attr)\n",
    "                self.g_cache[bin_code] = g\n",
    "            \n",
    "\n",
    "    def filter_delays(self, df:pd.DataFrame, dt=1/2):\n",
    "        bins = np.arange(0,24+dt,dt)\n",
    "        bins *= 60*60\n",
    "\n",
    "        df['bin'] = pd.cut(df['trip_start'], bins=bins, right=False)\n",
    "        df['bin_code'] = df['bin'].cat.codes\n",
    "        return df[df['bin_code'].isin(self.g_cache.keys())]\n",
    "\n",
    "dataset = TripDataset(\n",
    "    record_name='20251011',\n",
    "    data_path=data_path,\n",
    "    scalers=scalers\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndOfTripDelay(nn.Module):\n",
    "    def __init__(self, \n",
    "            in_dim: int, \n",
    "            gcn_dims:list[int], edge_attr_dim: int, \n",
    "            embd_dim: int,\n",
    "            mlp_dims: list[int],\n",
    "            out_dim: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.GCN = GCNWithEdge(\n",
    "            [in_dim] + gcn_dims + [embd_dim], \n",
    "            edge_attr_dim\n",
    "        )\n",
    "        \n",
    "        self.MLP = MLP(\n",
    "            [embd_dim + 2] + mlp_dims + [out_dim]\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        # 1. GCN (runs on the \"big graph\" efficiently)\n",
    "        x = self.GCN(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "        # 2. Filter nodes: only keep nodes that are part of the trips\n",
    "        x_trips = x[data.trip_mask]\n",
    "        # 3. Get the batch assignments for ONLY those trip nodes\n",
    "        # This is crucial so pooling knows which nodes belong to which trip in the batch\n",
    "        trip_batch = data.batch[data.trip_mask]\n",
    "        # 4. Average trip pooling (per graph in the batch)\n",
    "        # x will now have shape [batch_size, hidden_channels]\n",
    "        x = global_mean_pool(x_trips, trip_batch)\n",
    "\n",
    "        # 5. Meta embedding (day and sec are already [batch_size, 1])\n",
    "        day = data.day.view(-1, 1) \n",
    "        sec = data.sec.view(-1, 1)\n",
    "        x = torch.cat([x, day, sec], dim=-1)\n",
    "\n",
    "        # 6. MLP for prediction\n",
    "        return self.MLP(x)\n",
    "\n",
    "model = EndOfTripDelay(\n",
    "    in_dim=7,\n",
    "    gcn_dims=[8,16],\n",
    "    edge_attr_dim=2,\n",
    "    embd_dim=8,\n",
    "    mlp_dims=[6,6],\n",
    "    out_dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "std_dev = scalers['delay'].std\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # enumerate(loader) gives us the batch index (i)\n",
    "    for i, data in enumerate(loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.view(-1, 1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            abs_err = torch.abs(out - data.y.view(-1, 1))\n",
    "            # Multiply by std_dev to go from Z-score back to original units\n",
    "            real_mae = abs_err.mean().item() * std_dev\n",
    "\n",
    "        if (i + 1) % 1 == 0:\n",
    "            print(f\"Batch [{i+1}/{len(loader)}] | MSE (Z): {loss.item():.4f} | MAE (Real): {real_mae:.2f} s\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    print(f\"--- End of Epoch {epoch+1} | Average MSE: {avg_loss:.6f} ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
